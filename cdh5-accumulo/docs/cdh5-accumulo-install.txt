
################################################
# disable selinux
################################################
sed -i "s/SELINUX=enforcing/SELINUX=disabled/" /etc/selinux/config

################################################
# change vm swappiness
################################################
echo 2 > /proc/sys/vm/swappiness
echo '' >> /etc/sysctl.conf
echo 'vm.swappiness = 2' >> /etc/sysctl.conf

################################################
# turn off iptables
################################################
# turn iptables off
service iptables stop
service ip6tables stop
chkconfig iptables off
chkconfig ip6tables off

################################################
# setup passwordless ssh
################################################

# edit /etc/ssh/ssh_config and set StrictHostKeyChecking from ask to no
sed -i "s/# *StrictHostKeyChecking ask/StrictHostKeyChecking no" /etc/ssh/ssh_config

# create root ssh keys
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

# test keys by sshing without a password to localhost
ssh localhost
exit

################################################
# download java sdk and extract the rpm
################################################
cd ~/Downloads
wget http://download.oracle.com/otn-pub/java/jdk/7u75-b13/jdk-7u75-linux-x64.rpm
rpm -e jdk sun-javadb-common sun-javadb-core sun-javadb-client sun-javadb-demo sun-javadb-docs sun-javadb-javadoc

################################################
# create yum repo
################################################

# get cloudera cdh5 yum repo definitionon from cloudera
curl http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo > /etc/yum.repos.d/cloudera-cdh5.repo

# get cloudera accumulo yum repo defination from cloudera
curl http://archive.cloudera.com/accumulo-c5/redhat/6/x86_64/cdh/cloudera-accumulo.repo > /etc/yum.repos.d/cloudera-accumulo.repo

# if using priorities; add priorty=10 in the repo files => make sure he priority is higher than centos and lower than epel

# install apache
yum install -y httpd

# set checkconfig levels
chkconfig --levels 235 httpd on

# install required rpms for managing local yum repos
yum install -y yum-utils createrepo

# create a local yum repo of the chd4 repo
cd /var/www/html
mkdir yum
cd yum
reposync --repoid=cloudera-cdh5
cd cloudera-cdh5
cp ~/Downloads/jdk-7u67-linux-x64.rpm .
createrepo .

# create a local yum repo of accumulo
cd /var/www/html/yum
reposync --repoid=cloudera-accumulo
createrepo .

# start httpd
service httpd start

# update yum cdh5 repo definition
vi /etc/yum.repos.d/cloudera-cdh5.repo
# comment out the baseurl and gpgkey
# set the gpgcheck = 0
# create a new baseurl to http://localhost/yum/cloudera-cdh5

# update yum accumulo repo definition
vi /etc/yum.repos.d/cloudera-accumulo.repo
# comment out the baseurl and gpgkey
# set the gpgcheck = 0
# create a new baseurl to http://localhost/yum/cloudera-accumulo

# verify that the changes work
yum clean all
yum repolist

################################################
# install jdk, zookeeper, hadoop, accumulo, and other binaries
################################################
yum install -y jdk
yum install -y zookeeper-server
yum install -y hadoop-conf-pseudo hadoop-client
yum install -y sqoop
yum install -y hue hue-common
yum install -y accumulo-master accumulo-monitor accumulo-gc accumulo-tracer accumulo-tserver
yum install -y oozie oozie-client
yum install -y pig


################################################
# configure bashrc
################################################
export JAVA_HOME=/usr/java/default
export HADOOP_HOME=/usr/lib/hadoop
export HADOOP_CLIENT_HOME=/usr/lib/hadoop/client
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
export YARN_CONF_DIR=$HADOOP_CONF_DIR
export ACCUMULO_HOME=/usr/lib/accumulo
export ACCUMULO_CONF_DIR=/etc/accumulo/conf
export ZOOKEEPER_HOME=/usr/lib/zookeeper
export BASE_PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/bin:/usr/bin:/bin:$HOME/bin
export PATH=$BASE_PATH:$ACCUMULO_HOME/bin:$HADOOP_CLIENT_HOME/bin:$HADOOP_HOME/bin:$ZOOKEEPER_HOME/bin


###############################################
# configure hadoop
################################################

# export hadoop client home; this is need to use yarn
export HADOOP_CLIENT_HOME=/usr/lib/hadoop/client

# format namenode
sudo -u hdfs hdfs namenode -format

# to start hdfs from services
for x in $(cd /etc/init.d; ls hadoop-hdfs-*); do sudo service $x start; done

# create hdfs directories for hadoop
sudo -u hdfs hdfs dfs -mkdir -p /tmp/hadoop-yarn/staging/history/done_intermediate
sudo -u hdfs hdfs dfs -chown -R mapred:mapred /tmp/hadoop-yarn/staging
sudo -u hdfs hdfs dfs -chmod -R 1777 /tmp
sudo -u hdfs hdfs dfs -mkdir -p /var/log/hadoop-yarn
sudo -u hdfs hdfs dfs -chown yarn:mapred /var/log/hadoop-yarn
sudo -u hdfs hdfs dfs -mkdir -p /var/log/yarn
sudo -u hdfs hdfs dfs -chown yarn:mapred /var/log/yarn

# verify hdfs file structure
sudo -u hdfs hdfs dfs -ls -R /

# create user directories
sudo -u hdfs hdfs dfs -mkdir -p /user/root
sudo -u hdfs hdfs dfs -chown root /user/root
sudo -u hdfs hdfs dfs -chmod 755 /user/root

# start mapred from service
for x in $(cd /etc/init.d; ls hadoop-yarn-*); do sudo service $x start; done

# start history services
service hadoop-mapreduce-historyserver start

################################################
# configure zookeeper
################################################

# intialized zookeeper
service zookeeper-server init --myid 1

################################################
# configure accumulo
################################################

# verify the setting in /etc/accumulo/conf/accumulo-site.xml
# logger.dir.walog and instance.secret you may want to change
# if you change the logger.dir.walog make sure that directory
# is created on all nodes with mode of 1777
# WARNING => the memory setting may be too high for one node
# i would cut them in half if running a development vm under 8 gig

# verify the setting in /etc/default/accumulo
# WARNING => the memory setting expect to have 7+ gig of ram available
# i would cut them in half if running a development vm under 8 gigs
# you may need to reduce them

# create symbolic line for accumulo logs
cd $ACCUMULO_HOME
ln -s /var/log/accumulo logs

# create write ahead log
mkdir -p /var/lib/accumulo/walogs
chmod 1777 /var/lib/accumulo/walogs

# create accumulo directories in hdfs
sudo -u hdfs hdfs dfs -mkdir /accumulo
sudo -u hdfs hdfs dfs -chown accumulo /accumulo
sudo -u hdfs hdfs dfs -chmod 755 /accumulo

# delay accumulo service start to allow zookeeper and hadoop to start on boot
mv /etc/rc3.d/S85accumulo-master /etc/rc3.d/S99accumulo-master
mv /etc/rc4.d/S85accumulo-master /etc/rc4.d/S99accumulo-master
mv /etc/rc5.d/S85accumulo-master /etc/rc5.d/S99accumulo-master
mv /etc/rc3.d/S85accumulo-monitor /etc/rc3.d/S99accumulo-monitor
mv /etc/rc4.d/S85accumulo-monitor /etc/rc4.d/S99accumulo-monitor
mv /etc/rc5.d/S85accumulo-monitor /etc/rc5.d/S99accumulo-monitor
mv /etc/rc3.d/S85accumulo-gc /etc/rc3.d/S99accumulo-gc
mv /etc/rc4.d/S85accumulo-gc /etc/rc4.d/S99accumulo-gc
mv /etc/rc5.d/S85accumulo-gc /etc/rc5.d/S99accumulo-gc
mv /etc/rc3.d/S85accumulo-tracer /etc/rc3.d/S99accumulo-tracer
mv /etc/rc4.d/S85accumulo-tracer /etc/rc4.d/S99accumulo-tracer
mv /etc/rc5.d/S85accumulo-tracer /etc/rc5.d/S99accumulo-tracer
mv /etc/rc3.d/S85accumulo-tserver /etc/rc3.d/S99accumulo-tserver
mv /etc/rc4.d/S85accumulo-tserver /etc/rc4.d/S99accumulo-tserver
mv /etc/rc5.d/S85accumulo-tserver /etc/rc5.d/S99accumulo-tserver

# initialized accumulo master
service accumulo-master init

################################################
# start services
################################################

# start zookeeper service
service zookeeper-server start

# start hadoop services (these should already be started)
for x in $(cd /etc/init.d; ls hadoop-*); do service $x start; done

# start accumulo services
for x in $(cd /etc/init.d; ls accumulo-*); do service $x start; done

################################################
# configure hue
################################################
service hue start

#using browser goto http://localhost:8888

# setup user account
# username => root
# password => toor

################################################
# hadoop smoke test
################################################
# create a file with all the systems file structure
find / > find-data.txt
# replace all '/' with ' ' in find-data.txt
sed -i "s/\// /g" find-data.txt
hdfs dfs -mkdir find
hdfs dfs -mkdir find/input
hdfs dfs -put find-data.txt find/input
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount find/input find/output
hdfs dfs -cat find/output/part-r-00000 > /tmp/part-r-00000.txt
head -n 100 /tmp/part-r-00000.txt

################################################
# accumulo smoke test
################################################

# login to the accumulo shell
accumulo shell -u root -p toor

# create a table mytable
createtable mytable

# change to mytable
table mytable

# add records into mytable
insert -l public "john doe" contact phone 555-1212
insert -l public "john doe" contact address "123 somestreet"
insert -l public "john doe" contact city "sometown"

insert -l public "joe shmoe" contact phone 555-8989
insert -l public "joe shmoe" contact address "789 differentstreet"
insert -l public "joe shmoe" contact city "differenttown"

# scan table
scan

# get auths
getauths

# set auths to public
setauths -s public

# get auths
getauths

# scan to see new inputed records
scan


################################################
# cdh5 links
################################################
accumulo master    => http://localhost:50095
resource master    => http://localhost:8088/cluster
namenode           => http://localhost:50070
job history server => http://localhost:19888
hue server         => http://localhost:8888

